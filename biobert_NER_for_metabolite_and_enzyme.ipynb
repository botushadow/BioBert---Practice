{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biobert-NER for metabolite and enzyme",
      "provenance": [],
      "collapsed_sections": [
        "STeCPrW4KFpJ",
        "jtSnncB31FzW",
        "FtkOhSKAhM45"
      ],
      "mount_file_id": "1uULndqFUV9LXdSCwHKLBrXVWvtHXch5h",
      "authorship_tag": "ABX9TyMpUd4kJCn+qMtTQhj6nWg1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "icQKBKPTadOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea50a9d-f8ea-4848-ca18-37420c2450f5"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/13/b0ef494d53c6deba06d968286d79c084bbd186b3fe30ab48072d4b0923d0/boto3-1.17.77-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 16.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.77\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/ab/c4d8b0fa9a0e3b0bf6a17c778280a04486acd159b081f9f6a71c0cf74135/botocore-1.20.77-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.77->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.77->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.77 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.77 botocore-1.20.77 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 4.0MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STeCPrW4KFpJ"
      },
      "source": [
        "# Import BioBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9pgQv8Iffq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319c03b4-9448-4594-c6f1-bec08edd6779"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-21 15:54:11--  https://docs.google.com/uc?export=download&confirm=avI6&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.12.139, 108.177.12.138, 108.177.12.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.12.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/1bjk3rr5k3mvqmg6jvg41aeq67hsm40m/qlk48le7127ntv77u7s1bugsvqpnujj7/1621612425000/13799006341648886493/08477970927115493933Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download [following]\n",
            "--2021-05-21 15:54:11--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/1bjk3rr5k3mvqmg6jvg41aeq67hsm40m/qlk48le7127ntv77u7s1bugsvqpnujj7/1621612425000/13799006341648886493/08477970927115493933Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download\n",
            "Resolving doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
            "Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=7l49gpb3i5iie&continue=https://doc-08-ak-docs.googleusercontent.com/docs/securesc/1bjk3rr5k3mvqmg6jvg41aeq67hsm40m/qlk48le7127ntv77u7s1bugsvqpnujj7/1621612425000/13799006341648886493/08477970927115493933Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=mvblncapkdat7dchtusqs1723suljq6v [following]\n",
            "--2021-05-21 15:54:12--  https://docs.google.com/nonceSigner?nonce=7l49gpb3i5iie&continue=https://doc-08-ak-docs.googleusercontent.com/docs/securesc/1bjk3rr5k3mvqmg6jvg41aeq67hsm40m/qlk48le7127ntv77u7s1bugsvqpnujj7/1621612425000/13799006341648886493/08477970927115493933Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=mvblncapkdat7dchtusqs1723suljq6v\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.12.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/1bjk3rr5k3mvqmg6jvg41aeq67hsm40m/qlk48le7127ntv77u7s1bugsvqpnujj7/1621612425000/13799006341648886493/08477970927115493933Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=7l49gpb3i5iie&user=08477970927115493933Z&hash=ljrhbblngmtflhbnagv7d7mmpetncgme [following]\n",
            "--2021-05-21 15:54:12--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/1bjk3rr5k3mvqmg6jvg41aeq67hsm40m/qlk48le7127ntv77u7s1bugsvqpnujj7/1621612425000/13799006341648886493/08477970927115493933Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=7l49gpb3i5iie&user=08477970927115493933Z&hash=ljrhbblngmtflhbnagv7d7mmpetncgme\n",
            "Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘biobert_weights’\n",
            "\n",
            "biobert_weights         [                <=> ] 382.81M   103MB/s    in 3.7s    \n",
            "\n",
            "2021-05-21 15:54:16 (103 MB/s) - ‘biobert_weights’ saved [401403346]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDpKJWMBEa1G",
        "outputId": "b146ca5e-98e9-421b-d3f8-82fd33c7014a"
      },
      "source": [
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n",
            "model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5dhA43zEfrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2cd255-364f-4343-d535-d4920b6eadce"
      },
      "source": [
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-21 15:54:23.815187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcS0YX3qEwae"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbfnuT42EzR-"
      },
      "source": [
        "import transformers\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "train_batch_size = 8\n",
        "valid_batch_size = 4\n",
        "\n",
        "epochs = 12\n",
        "base_model_path = \"/content/biobert_v1.1_pubmed\"\n",
        "model_path = \"/content/biobert_appli_model.bin\"\n",
        "training_file = \"/content/training_metabolite_sep_check.csv\"\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "    base_model_path,\n",
        "    do_lower_case=True\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U8BIaHjFSn4"
      },
      "source": [
        "# DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBLjaG_LFVeo"
      },
      "source": [
        "import torch\n",
        "\n",
        "class EntityDataset:\n",
        "    def __init__(self, texts, tags, max_len):\n",
        "        # texts: [[\"hi\", \",\", \"my\", \"name\", \"is\" ...], [\"hello , ...\"]...]\n",
        "        # tags: [[1 2 3 4 1 5],[...]...]\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]  # item: [\"hi\", \",\", \"my\", \"name\", \"is\" ...]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_tag = []\n",
        "\n",
        "        for i, s in enumerate(text):  # enumerate(text): [(0, \"hi\"), (1, \",\"), (2, \"my\"), ...]\n",
        "            inputs = tokenizer.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # shadow: sh ##ad ##sh ##ek (if \"shadow\" not in the dictionary)\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs)\n",
        "            target_tag.extend([tags[i]] * input_len)\n",
        "\n",
        "        ids = ids[:self.max_len - 2]\n",
        "        target_tag = target_tag[:self.max_len - 2]\n",
        "\n",
        "        ids = [101] + ids + [102]  # 2 for special dots\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = self.max_len - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len)\n",
        "        mask = mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "                \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "                \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "                \"target_tag\": torch.tensor(target_tag, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4RBnuAVFuPf"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1oA416tF0yE"
      },
      "source": [
        "# import config\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "\n",
        "\n",
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    # ==== acc ====\n",
        "    acc = 0\n",
        "    word_num = 0\n",
        "    pred_pre = torch.argmax(active_logits, dim=1)\n",
        "    pred = torch.where(\n",
        "        active_loss,\n",
        "        pred_pre.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(pred_pre)\n",
        "    )\n",
        "    for i in range(len(list(active_labels))):\n",
        "        if list(active_labels)[i] == 0 or list(active_labels)[i] == 1 or list(active_labels)[i] == 2:\n",
        "            word_num = word_num + 1\n",
        "            if list(active_labels)[i] == list(pred)[i]:\n",
        "                acc = acc + 1\n",
        "\n",
        "    return loss, acc, word_num\n",
        "\n",
        "\n",
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag):  #, bert_state_dict):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.bert = transformers.BertModel.from_pretrained(base_model_path)\n",
        "        '''self.bert = BertModel(bert_config_file)\n",
        "        if bert_state_dict is not None:\n",
        "            self.bert.load_state_dict(bert_state_dict)'''\n",
        "        self.bert_drop_1 = nn.Dropout(0.4)\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "\n",
        "        bo_tag = self.bert_drop_1(o1)\n",
        "\n",
        "        tag = self.out_tag(bo_tag)\n",
        "\n",
        "        loss_tag, acc, word_num = loss_fn(tag, target_tag, mask, self.num_tag)\n",
        "\n",
        "        loss = loss_tag\n",
        "\n",
        "        return tag, loss, acc, word_num"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70qIiX5SGPpz"
      },
      "source": [
        "# Engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKloTkTDGSp5"
      },
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    accuracy = 0\n",
        "    word_nums = 0\n",
        "    # num_correct = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, loss, acc, word_num = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "        accuracy = accuracy + acc\n",
        "        word_nums = word_nums + word_num\n",
        "\n",
        "    return final_loss / len(data_loader), accuracy / word_nums\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    accuracy = 0\n",
        "    word_nums = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        _, loss, acc, word_num = model(**data)\n",
        "        final_loss += loss.item()\n",
        "        accuracy = accuracy + acc\n",
        "        word_nums = word_nums + word_num\n",
        "\n",
        "    return final_loss / len(data_loader), accuracy / word_nums"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGlRTtznfV_L"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rGF4KclQDHcA",
        "outputId": "3e56cb01-cdd9-40ff-ad8e-f6822a57ce00"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import host_subplot\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "# ====为了让模型实验结果尽可能一致====\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(1)\n",
        "\n",
        "\n",
        "def process_data(data_path):\n",
        "    # ==== 把csv中单词和标签分开存放 ====\n",
        "    instances = open(data_path, encoding='unicode_escape').read().strip().split('\\n,,\\n,,\\n')\n",
        "    # print(instances)\n",
        "    word = []\n",
        "    tag = []\n",
        "    file_num = []\n",
        "\n",
        "    count = 0\n",
        "    for i in instances:\n",
        "        if count < 80:\n",
        "            count = count + 1\n",
        "            file_words_tags = i.split('\\n')\n",
        "\n",
        "            for j in file_words_tags:\n",
        "                file_num.append(j.split(',')[0])\n",
        "                word.append(j.split(',')[1])\n",
        "                tag.append(j.split(',')[2])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # ==== dataframe ====\n",
        "    data = {'File#': file_num, 'Word': word, 'Tag': tag}\n",
        "    df = pd.DataFrame(data=data)\n",
        "    print(\"df complete!\")\n",
        "    print(df.head(200))\n",
        "\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentence = df.groupby(\"File#\")[\"Word\"].apply(list).values\n",
        "    tags = df.groupby(\"File#\")[\"Tag\"].apply(list).values\n",
        "\n",
        "    return sentence, tags, enc_tag\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentence, tags, enc_tag = process_data(training_file)\n",
        "\n", 
        "    max_len = 0\n",
        "    for i in sentence:\n",
        "        if len(i) > max_len:\n",
        "            max_len = len(i)\n",
        "    print(\"FileSentence_max_len\", max_len, \"File_num\", len(sentence))\n",
        "\n",
        "    # ==== 载入数据 ====\n",
        "    meta_data = {\n",
        "        \"enc_tag\": enc_tag\n",
        "    }\n",
        "    joblib.dump(meta_data, \"meta.bin\")  #\n",
        "\n",
        "    num_tag = len(list(enc_tag.classes_))  # attributes: classes_  ndarray of shape (n_classes,)\n",
        "\n",
        "    (\n",
        "        train_sentences,\n",
        "        test_sentences,\n",
        "        train_tag,\n",
        "        test_tag\n",
        "    ) = model_selection.train_test_split(sentence, tags, random_state=42, test_size=0.1)\n",
        "\n",
        "    train_dataset = EntityDataset(\n",
        "         texts=train_sentences, tags=train_tag, max_len=max_len\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "         train_dataset, batch_size=train_batch_size, num_workers=0\n",
        "    )\n",
        "    print(\"train_data load over!\")\n",
        "\n",
        "    valid_dataset = EntityDataset(\n",
        "        texts=test_sentences, tags=test_tag, max_len=max_len\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=valid_batch_size, num_workers=0\n",
        "    )\n",
        "    print(\"valid_data load over!\")\n",
        "\n",
        "    # ==== 载入模型 ====\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = EntityModel(num_tag=num_tag)\n",
        "    model.to(device)\n",
        "    print(\"Model load!\")\n",
        "\n",
        "    # ==== 载入engine训练 ====\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.001,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    num_train_steps = int(len(train_sentences) / train_batch_size * epochs)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_loss = np.inf\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    train_acc_list = []\n",
        "    test_acc_list = []\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "        test_loss, test_acc = eval_fn(valid_data_loader, model, device)\n",
        "        train_loss_list.append(train_loss)\n",
        "        test_loss_list.append(test_loss)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "        print(f\"Train Acc = {train_acc} Valid Acc = {test_acc}\")\n",
        "        if test_loss < best_loss:\n",
        "            torch.save(model.state_dict(), model_path, _use_new_zipfile_serialization=False)\n",
        "            best_loss = test_loss\n",
        "\n",
        "    # ==== 画图 ====\n",
        "    plt.subplot(121)\n",
        "    epochs = range(len(train_loss_list))\n",
        "    plt.plot(epochs, train_loss_list, 'b', label='train_loss')\n",
        "    plt.plot(epochs, test_loss_list, 'r', label='test_loss')\n",
        "\n",
        "    plt.legend()\n  # loss曲线应当也加上图注",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.plot(epochs, train_acc_list, 'b', label='train_acc')\n",
        "    plt.plot(epochs, test_acc_list, 'r', label='test_acc')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(\"Histogram of loss and accuracy\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df complete!\n",
            "     File#                 Word         Tag\n",
            "0    file1  glucose-6-phosphate      enzyme\n",
            "1    file1        dehydrogenase      enzyme\n",
            "2    file1   6-phosphogluconate  metabolite\n",
            "3    file1        dehydrogenase      enzyme\n",
            "4    file1          glutathione  metabolite\n",
            "..     ...                  ...         ...\n",
            "195  file3                 lead       other\n",
            "196  file3                   to       other\n",
            "197  file3             multiple       other\n",
            "198  file3            enzymatic       other\n",
            "199  file3               losses       other\n",
            "\n",
            "[200 rows x 3 columns]\n",
            "FileSentence_max_len 341 File_num 80\n",
            "train_data load over!\n",
            "valid_data load over!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model load!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:03<00:00, 53.69s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.67s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.6266498797469668 Valid Loss = 0.46067066490650177\n",
            "Train Acc = 0.7707219644011044 Valid Acc = 0.8107142857142857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:04<00:00, 53.78s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.84s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.3216111577219433 Valid Loss = 0.27786320447921753\n",
            "Train Acc = 0.8624801738823944 Valid Acc = 0.8852040816326531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:04<00:00, 53.83s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.85s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.2404997083875868 Valid Loss = 0.2490217611193657\n",
            "Train Acc = 0.9078305821535569 Valid Acc = 0.898469387755102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:03<00:00, 53.77s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.81s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.1859377059671614 Valid Loss = 0.20224644988775253\n",
            "Train Acc = 0.931386947071609 Valid Acc = 0.9198979591836735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:03<00:00, 53.70s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.85s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.14355244901445177 Valid Loss = 0.18401197344064713\n",
            "Train Acc = 0.9483639781472126 Valid Acc = 0.9270408163265306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:05<00:00, 53.95s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.83s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.11454159600867166 Valid Loss = 0.16443436592817307\n",
            "Train Acc = 0.9591728837455208 Valid Acc = 0.9295918367346939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:06<00:00, 54.01s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.80s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.09271910124354893 Valid Loss = 0.16421787440776825\n",
            "Train Acc = 0.9690418845091935 Valid Acc = 0.9331632653061225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:03<00:00, 53.69s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.84s/it]\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.07380086556077003 Valid Loss = 0.17230584472417831\n",
            "Train Acc = 0.9770898196557598 Valid Acc = 0.9346938775510204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:02<00:00, 53.60s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.84s/it]\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.06480502585570018 Valid Loss = 0.16943359375\n",
            "Train Acc = 0.9814956235681137 Valid Acc = 0.9413265306122449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:01<00:00, 53.45s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.85s/it]\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.058081926984919444 Valid Loss = 0.17350831627845764\n",
            "Train Acc = 0.9840803618633613 Valid Acc = 0.9387755102040817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:05<00:00, 53.91s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.82s/it]\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.052327452020512685 Valid Loss = 0.17525187134742737\n",
            "Train Acc = 0.985196498854491 Valid Acc = 0.9403061224489796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [08:01<00:00, 53.48s/it]\n",
            "100%|██████████| 2/2 [00:19<00:00,  9.82s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.05017068940732214 Valid Loss = 0.17665404826402664\n",
            "Train Acc = 0.9865476120542795 Valid Acc = 0.9387755102040817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZO9phN3eivG"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quk9-XJ6MUvz",
        "outputId": "374fdcae-2247-4396-cfac-5a1bffde885c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import host_subplot\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # meta_data = joblib.load(\"meta.bin\")\n",
        "    meta_data = joblib.load(\"/content/drive/MyDrive/Biobert_metabolite_result_weights/80_files_meta.bin\")\n",
        "    enc_tag = meta_data[\"enc_tag\"]\n",
        "\n",
        "    num_tag = len(list(enc_tag.classes_))  # attributes: classes_  ndarray of shape (n_classes,)\n",
        "\n",
        "    sentence = '''Glucose-6-phosphate dehydrogenase, 6-phosphogluconate dehydrogenase, glutathione reductase and pyruvate kinase of Candida \n",
        "    utilis and baker's yeast, when in anionic form, were adsorbed on a cation exchanger, P-cellulose, due to affinities similar to \n",
        "    those for the phosphoric groups of their respective substrates; thus, glucose-6-phosphate dehydrogenase was readily eluted by either \n",
        "    or NADPH, glutathione reductase by NADPH, 6-phosphogluconate dehydrogenase by 6-phosphogluconate, and pyruvate kinase by either ATP \n",
        "    or ADP. This type of chromatography may be called \"affinity-adsorption-elution chromatography\"; the main principle is different from \n",
        "    that of so-called affinity-elution chromatography. Based on these findings, a large-scale procedure suitable for successive purification \n",
        "    of several enzymes having affinities for the phosphoric groups of their substrates was devised. As an example, glucose-6-phosphate \n",
        "    dehydrogenase was highly purified from baker's yeast and crystallized.'''\n",
        "\n",
        "\n",
        "    tokenized_uncode = tokenizer.tokenize(\n",
        "        sentence,\n",
        "    )\n",
        "\n",
        "    tokenized_sentence = tokenizer.encode(\n",
        "           sentence,\n",
        "    )\n",
        "\n",
        "    sentence = sentence.split()\n",
        "    max_len = len(tokenized_uncode)\n",
        "\n",
        "\n",
        "    test_dataset = EntityDataset(\n",
        "       texts=[sentence],\n",
        "       tags=[[0] * len(sentence)],\n",
        "       max_len=max_len\n",
        "    )\n",
        "\n",
        "    # ==== 载入模型 ====\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if use cpu to run\n",
        "    model = EntityModel(num_tag=num_tag)\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/Biobert_metabolite_result_weights/80_files_biobert_appli_model.bin'))\n",
        "    model.to(device)\n",
        "    print(\"Model load!\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data = test_dataset[0]\n",
        "\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device).unsqueeze(0)\n",
        "        tag, _, _, _ = model(**data)\n",
        "\n",
        "        prediction = enc_tag.inverse_transform(\n",
        "                tag.argmax(2).cpu().numpy().reshape(-1)\n",
        "            )[:len(tokenized_sentence)]\n",
        "\n",
        "        list_pred = []\n",
        "        for i in tokenized_uncode:\n",
        "            list_pred.append(str(i))\n",
        "        list_pred = [101] + list_pred + [102]\n",
        "        print(list_pred)\n",
        "        pred_dic = {}\n",
        "        for i in range(len(prediction)):\n",
        "            pred_dic[list_pred[i]] = prediction[i]\n",
        "        print(pred_dic)\n",
        "\n",
        "# ==== NER模型分词及权重导入参考https://github.com/abhi1thakur/bert-sentiment及https://towardsdatascience.com/tagging-genes-and-proteins-with-biobert-c7b04fc6eb4f ===="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model load!\n",
            "[101, 'glucose', '-', '6', '-', 'phosphate', 'de', '##hy', '##dr', '##ogen', '##ase', ',', '6', '-', 'p', '##hos', '##ph', '##og', '##lu', '##cona', '##te', 'de', '##hy', '##dr', '##ogen', '##ase', ',', 'g', '##lut', '##ath', '##ione', 'red', '##uc', '##tase', 'and', 'p', '##yr', '##u', '##vate', 'kinase', 'of', 'can', '##di', '##da', 'u', '##til', '##is', 'and', 'b', '##aker', \"'\", 's', 'yeast', ',', 'when', 'in', 'an', '##ion', '##ic', 'form', ',', 'were', 'ads', '##or', '##bed', 'on', 'a', 'cat', '##ion', 'exchange', '##r', ',', 'p', '-', 'cell', '##ulos', '##e', ',', 'due', 'to', 'a', '##ffin', '##ities', 'similar', 'to', 'those', 'for', 'the', 'p', '##hos', '##ph', '##ori', '##c', 'groups', 'of', 'their', 'respective', 'substrates', ';', 'thus', ',', 'glucose', '-', '6', '-', 'phosphate', 'de', '##hy', '##dr', '##ogen', '##ase', 'was', 'readily', 'el', '##uted', 'by', 'either', 'or', 'na', '##d', '##ph', ',', 'g', '##lut', '##ath', '##ione', 'red', '##uc', '##tase', 'by', 'na', '##d', '##ph', ',', '6', '-', 'p', '##hos', '##ph', '##og', '##lu', '##cona', '##te', 'de', '##hy', '##dr', '##ogen', '##ase', 'by', '6', '-', 'p', '##hos', '##ph', '##og', '##lu', '##cona', '##te', ',', 'and', 'p', '##yr', '##u', '##vate', 'kinase', 'by', 'either', 'at', '##p', 'or', 'ad', '##p', '.', 'this', 'type', 'of', 'ch', '##roma', '##tography', 'may', 'be', 'called', '\"', 'affinity', '-', 'ads', '##or', '##ption', '-', 'el', '##ution', 'ch', '##roma', '##tography', '\"', ';', 'the', 'main', 'principle', 'is', 'different', 'from', 'that', 'of', 'so', '-', 'called', 'affinity', '-', 'el', '##ution', 'ch', '##roma', '##tography', '.', 'based', 'on', 'these', 'findings', ',', 'a', 'large', '-', 'scale', 'procedure', 'suitable', 'for', 'successive', 'pu', '##rification', 'of', 'several', 'enzymes', 'having', 'a', '##ffin', '##ities', 'for', 'the', 'p', '##hos', '##ph', '##ori', '##c', 'groups', 'of', 'their', 'substrates', 'was', 'devised', '.', 'as', 'an', 'example', ',', 'glucose', '-', '6', '-', 'phosphate', 'de', '##hy', '##dr', '##ogen', '##ase', 'was', 'highly', 'pu', '##rified', 'from', 'b', '##aker', \"'\", 's', 'yeast', 'and', 'crystal', '##li', '##zed', '.', 102]\n",
            "{101: 'enzyme', 'glucose': 'metabolite', '-': 'metabolite', '6': 'metabolite', 'phosphate': 'metabolite', 'de': 'enzyme', '##hy': 'enzyme', '##dr': 'enzyme', '##ogen': 'enzyme', '##ase': 'enzyme', ',': 'other', 'p': 'other', '##hos': 'other', '##ph': 'other', '##og': 'metabolite', '##lu': 'metabolite', '##cona': 'metabolite', '##te': 'metabolite', 'g': 'enzyme', '##lut': 'enzyme', '##ath': 'enzyme', '##ione': 'enzyme', 'red': 'enzyme', '##uc': 'enzyme', '##tase': 'enzyme', 'and': 'other', '##yr': 'metabolite', '##u': 'metabolite', '##vate': 'metabolite', 'kinase': 'enzyme', 'of': 'other', 'can': 'other', '##di': 'other', '##da': 'other', 'u': 'other', '##til': 'other', '##is': 'other', 'b': 'other', '##aker': 'other', \"'\": 'other', 's': 'other', 'yeast': 'other', 'when': 'other', 'in': 'other', 'an': 'other', '##ion': 'other', '##ic': 'other', 'form': 'other', 'were': 'other', 'ads': 'other', '##or': 'other', '##bed': 'other', 'on': 'other', 'a': 'other', 'cat': 'other', 'exchange': 'other', '##r': 'other', 'cell': 'other', '##ulos': 'other', '##e': 'other', 'due': 'other', 'to': 'other', '##ffin': 'other', '##ities': 'other', 'similar': 'other', 'those': 'other', 'for': 'other', 'the': 'other', '##ori': 'other', '##c': 'other', 'groups': 'other', 'their': 'other', 'respective': 'other', 'substrates': 'other', ';': 'other', 'thus': 'other', 'was': 'other', 'readily': 'other', 'el': 'other', '##uted': 'other', 'by': 'other', 'either': 'other', 'or': 'other', 'na': 'other', '##d': 'other', 'at': 'other', '##p': 'other', 'ad': 'other', '.': 'other', 'this': 'other', 'type': 'other', 'ch': 'other', '##roma': 'other', '##tography': 'other', 'may': 'other', 'be': 'other', 'called': 'other', '\"': 'other', 'affinity': 'other', '##ption': 'other', '##ution': 'other', 'main': 'other', 'principle': 'other', 'is': 'other', 'different': 'other', 'from': 'other', 'that': 'other', 'so': 'other', 'based': 'other', 'these': 'other', 'findings': 'other', 'large': 'other', 'scale': 'other', 'procedure': 'other', 'suitable': 'other', 'successive': 'other', 'pu': 'other', '##rification': 'other', 'several': 'other', 'enzymes': 'other', 'having': 'other', 'devised': 'other', 'as': 'other', 'example': 'other', 'highly': 'other', '##rified': 'other', 'crystal': 'other', '##li': 'other', '##zed': 'enzyme'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao4DGlh2YRPR"
      },
      "source": [
        "# Confusion Matrix - onefile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz6EppUQYRBi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "41de9ce4-7600-40f9-e789-439f835c7818"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import host_subplot\n",
        "\n",
        "from sklearn.metrics import confusion_matrix  # y_true = [,...,]  y_pred = [, ... ,]\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(1)\n",
        "\n",
        "# ==== 读取数据 ====\n",
        "def process_data(data_path, enc_tag):\n",
        "    # ==== 把tsv中单词和标签分开存放 ====\n",
        "    instances = open(data_path).read().strip().split('\\n,,\\n,,\\n')\n",
        "\n",
        "    word = []\n",
        "    tag = []\n",
        "    file_num = []\n",
        "\n",
        "    count = 0\n",
        "    for i in instances:\n",
        "      count = count + 1\n",
        "      if count == 1:  \n",
        "          file_words_tags = i.split('\\n')\n",
        "          for j in file_words_tags:\n",
        "              file_num.append(j.split(',')[0])\n",
        "              word.append(j.split(',')[1])\n",
        "              tag.append(j.split(',')[2])\n",
        "      else:\n",
        "          break  # ==== 句子不能过长…… ===='\n",
        "\n",
        "    return word, tag\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    meta_data = joblib.load(\"/content/drive/MyDrive/Biobert_metabolite_result_weights/80_files_meta.bin\")\n",
        "    # meta_data = joblib.load(\"/content/meta.bin\")\n",
        "    enc_tag = meta_data[\"enc_tag\"]\n",
        "    num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "    # ==== 加载测试集 ====\n",
        "    # word, tags = process_data('/content/testing_metabolite_sep_check_one_file.csv', enc_tag)\n",
        "    word, tags = process_data('/content/testing_metabolite_sep_check_file_296.csv', enc_tag)\n",
        "\n",
        "    if \"enzyme\" not in tags:\n",
        "      tags.append(\"enzyme\")\n",
        "      word.append(\"glucoamylase\")\n",
        "    if \"metabolite\" not in tags:\n",
        "      tags.append(\"metabolite\")\n",
        "      word.append(\"glucose\")\n",
        "    target_tags = enc_tag.fit_transform(tags)\n",
        "\n",
        "\n",
        "    string = \" \".join(word)\n",
        "\n",
        "\n",
        "    tokenized_sentence = tokenizer.encode(\n",
        "           string,\n",
        "    )\n",
        "\n",
        "    max_len = len(tokenized_sentence)\n",
        "\n",
        "    test_dataset = EntityDataset(\n",
        "         texts=[word], tags=[[0] * len(word)], max_len=max_len\n",
        "    )\n",
        "\n",
        "    print('Test data load over!')\n",
        "\n",
        "\n",
        "    # ==== 载入模型 ====\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if use cpu to run\n",
        "    model = EntityModel(num_tag=num_tag)\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/Biobert_metabolite_result_weights/80_files_biobert_appli_model.bin'))\n",
        "    # model.load_state_dict(torch.load('/content/biobert_appli_model.bin'))\n",
        "    model.to(device)\n",
        "    print(\"Model load!\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data = test_dataset[0]\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device).unsqueeze(0)\n",
        "        tag, _, _, _= model(**data)\n",
        "        prediction = enc_tag.inverse_transform(\n",
        "                tag.argmax(2).cpu().numpy().reshape(-1)\n",
        "            )\n",
        "        pred = tag.argmax(2).cpu().numpy().reshape(-1)[:len(tokenized_sentence)]\n",
        "\n",
        "      \n",
        "    # ==== target_tags ====\n",
        "    true_tags = list(target_tags)\n",
        "    target_tags = []\n",
        "    for i, s in enumerate(word):\n",
        "        inputs = tokenizer.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "        input_len = len(inputs)\n",
        "        target_tags.extend([true_tags[i]] * input_len)\n",
        "\n",
        "    target_tags = target_tags[:max_len - 2]\n",
        "    target_tags = [2] + target_tags + [2]\n",
        "\n",
        "    # ==== 0: enzymes, 1:metabolite, 2: other ====\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for m in pred:\n",
        "      if m == 2:\n",
        "        y_pred.append('other')\n",
        "      if m == 1:\n",
        "        y_pred.append('metabolite')\n",
        "      if m == 0:\n",
        "        y_pred.append('enzyme')\n",
        "    for n in target_tags:\n",
        "      if str(n) == '2':\n",
        "        y_true.append('other')\n",
        "        # count_others = count_others+1\n",
        "      if str(n) == '1':\n",
        "        y_true.append('metabolite')\n",
        "      if str(n) == '0':\n",
        "        y_true.append('enzyme')\n",
        "    # print(count_others)\n",
        "\n",
        "    # confusion_mat = confusion_matrix(y_true, y_pred, labels=[\"enzyme\", \"metabolite\", \"other\"]) # 这里顺序不能错，0、1、2\n",
        "    confusion_mat = confusion_matrix(target_tags, pred, labels=[0, 1, 2])\n",
        "\n",
        "    # ==== 混淆矩阵画图 参看https://deeplizard.com/learn/video/0LhiS6yu2qQ====\n",
        "    cm = confusion_mat\n",
        "    def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "      if normalize:\n",
        "          cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "          print(\"Normalized confusion matrix\")\n",
        "      else:\n",
        "          print('Confusion matrix, without normalization')\n",
        "\n",
        "      print(cm)\n",
        "      plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "      plt.title(title)\n",
        "      plt.colorbar()\n",
        "      tick_marks = np.arange(len(classes))\n",
        "      plt.xticks(tick_marks, classes, rotation=45)\n",
        "      plt.yticks(tick_marks, classes)\n",
        "\n",
        "      fmt = '.2f' if normalize else 'd'\n",
        "      thresh = cm.max() / 2.\n",
        "      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "          plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "    \n",
        "    classes = ['1-enzyme', '2-metabolie', '3-other']\n",
        "    plot_confusion_matrix(cm, classes)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data load over!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/biobert_v1.1_pubmed were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model load!\n",
            "Confusion matrix, without normalization\n",
            "[[  0   0   5]\n",
            " [  5  80   3]\n",
            " [  2   9 166]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
